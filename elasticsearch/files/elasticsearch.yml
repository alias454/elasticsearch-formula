#File managed via Salt do not edit
{% from "elasticsearch/map.jinja" import host_lookup as config with context %}
# ======================== Elasticsearch Configuration =========================
#
# NOTE: Elasticsearch comes with reasonable defaults for most settings.
#       Before you set out to tweak and tune the configuration, make sure you
#       understand what are you trying to accomplish and the consequences.
#
# The primary way of configuring a node is via this file. This template lists
# the most important settings you may want to configure for a production cluster.
#
# Please consult the documentation for further information on configuration options:
# https://www.elastic.co/guide/en/elasticsearch/reference/index.html
#
# ---------------------------------- Cluster -----------------------------------
#
# Use a descriptive name for your cluster:
#
cluster.name: {{ config.elasticsearch.cluster_name }}

#
# ------------------------------------ Node ------------------------------------
#
# Use a descriptive name for the node:
#
node.name: {{ grains['host'] }}

#
# Allow this node to be eligible as a master node (enabled by default):
#
node.master: {{ config.elasticsearch.master_node }}

#
# Allow this node to store data (enabled by default):
#
node.data: {{ config.elasticsearch.data_node }}

#
# Add custom attributes to the node:
#
# node.attr.rack: r1

#
# ----------------------------------- Paths ------------------------------------
#
# Path to directory where to store the data (separate multiple locations by comma):
#
path.data: {{ config.elasticsearch.data_path }}

#
# Path to log files:
#
path.logs: {{ config.elasticsearch.log_path }}

#
# ----------------------------------- Memory -----------------------------------
#
# Make sure that the `ES_HEAP_SIZE` environment variable is set to about half the memory
# available on the system and that the owner of the process is allowed to use this limit.
#
# Elasticsearch performs poorly when the system is swapping the memory.
# Lock the memory on startup:
#
bootstrap.memory_lock: {{ config.elasticsearch.mlockall }}

# bootstrap.mlockall: {{ config.elasticsearch.mlockall }}

#
# ---------------------------------- Network -----------------------------------
#
# Set the bind address to a specific IP (IPv4 or IPv6):
#
network.host: {{ config.elasticsearch.network_host }}

# Set a custom port for HTTP:
#
# http.port: 9200

#
# --------------------------------- Discovery ----------------------------------
#
# Pass an initial list of hosts to perform discovery when new node is started:
# The default list of hosts is ["127.0.0.1", "[::1]"]
#
# For more information, consult the network module documentation.
#
discovery.zen.ping.unicast.hosts: {{ config.elasticsearch.unicast_hosts }}

#
# Prevent the "split brain" by configuring the majority of nodes (total number of master-eligible nodes / 2 + 1):
#
# discovery.zen.minimum_master_nodes: 3
#
# For more information, consult the zen discovery module documentation.

#
# --------------------------- Recovery Throttling ------------------------------
#
# These settings allow to control the process of shards allocation between
# nodes during initial recovery, replica allocation, rebalancing,
# or when adding and removing nodes.

# Set the number of concurrent recoveries happening on a node:
# During the initial recovery
#
cluster.routing.allocation.node_initial_primaries_recoveries: {{ config.elasticsearch.initial_primaries_recoveries }}

# During adding/removing nodes, rebalancing, etc
#
cluster.routing.allocation.node_concurrent_recoveries: {{ config.elasticsearch.concurrent_recoveries }}

# Set to throttle throughput when recovering (eg. 100mb, by default 20mb):
#
indices.recovery.max_bytes_per_sec: {{ config.elasticsearch.recovery_max_bytes }}

# Set to limit the number of open concurrent streams when
# recovering a shard from a peer:
#
#indices.recovery.concurrent_streams: {{ config.elasticsearch.recovery_concurrent_streams }}

#
# ---------------------------------- Gateway -----------------------------------
#
# Block initial recovery after a full cluster restart until N nodes are started:
#
# gateway.recover_after_nodes: 3
#
# For more information, consult the gateway module documentation.

#
# ---------------------------------- Various -----------------------------------
#
# Disable starting multiple nodes on a single system:
#
# node.max_local_storage_nodes: 1
#
# Require explicit names when deleting indices:
#
# action.destructive_requires_name: true
